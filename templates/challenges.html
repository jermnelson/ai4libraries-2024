{% extends "base.html" %}


{% block main %}
<div>
 <p>
   There are multiple challenges associated with using AI and  
   more specifically Large Language Models (LLMs) like <a href="https://chatgpt.com/">ChatGPT</a>
   and <a href="https://gemini.google.com/">Google's Gemini</a>, some of which can
   be minimized.
 </p>
</div>
<div class="row">
  <div class="col-4">
    <h1>Privacy Concerns in Large Language Models</h1>
     <p>
      With the widespread release of Large Language Models (LLMs) by various  organizations,
      <a href="https://arxiv.org/abs/2310.10383">ignificant privacy issues</a> including:</p>
     <ul>
       <li><strong>Personal details in training data</strong>: Names, addresses, and financial information may
       be included in the training data for these models. </li>
      <li><strong>Logged prompts containing sensitive information</strong>: User inputs, including private 
      details, can be logged by the companies managing these LLMs. </li>
      <li><strong>Re-identification of individuals:</strong> Even in anonymized training data, individuals can potentially be
       re-identified through the model outputs and usage patterns.</li>
     </ul>
  </div>
  <div class="col-4">
   <h3>Bias</h3>
   <p>AI bias occurs when models produce outputs that reflect or 
      perpetuate existing inequalities and perspectives in the larger
      society.</p>
   <h4>Sources</h4>
   <ul>
     <li><strong>Training data</strong> -- over or under sample underrepresented groups, biased in 
         labeling by excluding or over-representing certain categories or characteristics.
     </li>
     <li><strong>Algorithms</strong> -- based on biased data could perpetrate underlying flaw, programmers 
         introduce personal bias intentionally or unintentionally
     </li>
     <li><strong>Proxies</strong> -- Unintended consequences of using proxies for characteristics in the population</li>
     <li><strong>Cognitive </strong>-- people's experiences and preferences can introduce and favor bias or 
  weighting of outcomes or selection of data.</li>
   </ul>
  </div>
  <div class="col-4">
   <h3>Hallucinations</h3>
   <p>
     Since the initial release of ChatGPT 3.5 in 2022, a major criticism of Large 
     Language Models (LLMs) has been the tendency of these models to fabricate factually
     incorrect statements. LLMs generate text by predicting the most likely continuation token
     based on the prompt's text, context, and model internal weights. Unlike a deductive process, LLMs
     do not directly reference their training source material to generate responses. 
   </p>
   <h4>Types</h4>
   <ul>
    <li><strong>Fact-conflicting</strong> -- 1+1 = 3</li>
    <li><strong>Input-conflicting</strong> -- LLM summarizes an article and includes details not present
     in the original article.</li>
    <li><strong>Context-conflicting</strong> -- inconsistent or self-contradiction in output 
   </ul>
   <h4>Mitigation</h4>
   <ul>
     <li><strong>Chain-of-thought (COT)</strong>: A technique prompting the model to break down its reasoning process
     into sequential steps,explaining how it arrived to its final answer.</li>
     <li><strong>One-shot and Few-shot Prompts</strong>: Techniques that provide context by offering sample responses in 
      a given format, enabling the model to infer patterns for consistency and accuracy in its
      answers.</li>
     <li><strong>Retrieval Augmented Generation (RAG)</strong>: Combines contextual examples with the prompt, grounding the
     model in factual, current material from external sources, and reducing the model's dependence on 
      outdated or incomplete information training data.</li>
    <li><strong>Reinforcement Learning with Human Feedback (RLHF)</strong>: A fine-tuning technique of adding direct human 
     feedback to a model's responses by rewarding factual responses and penalizing hallucinations.  </li>
   </ul>
  </div>
</div>
<div class="row">
  <div class="col-4">
   <h3>Academic Fraud &amp; Copyright</h3>
    <p>
     One of the immediate concerns regarding academic fraud is the use of 
     Large Language Models (LLMs) that generate convincing and coherent text that
     students and researchers can pass off as original work. The growth and inclusion 
     of generative text into academic articles has been widespread, particularly in 
     the computer science literature (<a href="https://hai.stanford.edu/news/how-much-research-being-written-large-language-models">How Much Research Is Being Written by Large Language Models?</a>)
    </p>
   <p>
     The training of Large Language Models (LLMs) requires massive amounts of text and other 
     media that are commonly available on the open web. This content includes both copyrighted 
     and public domain material, which can lead to generative outputs from these models closely 
     resembling existing copyrighted works.
   </p>
  </div>
  <div class="col-4">
   <h3>Carbon Footprint</h3>
   <p>
     A real concern of Large Language Models (LLMs) is the amount of energy and water 
     required for training and deploying these models. For example, in their 2024 
     <a href="https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf">report</a>
     Google admitted that their carbon output increased over 13% year-over-year primarily due to the 
     increased energy usage of their customer-facing AI efforts, including the training and inference
     of their flagship <a href="https://gemini.google.com/">Gemini LLM</a>.
   </p>
   <p>In a 2024 <a href="https://techcommunity.microsoft.com/t5/azure-architecture-blog/reducing-the-environmental-impact-of-generative-ai-a-guide-for/">report</a> released by Microsoft, four suggestions are presented to help 
    Generative AI developers 
    and users of these LLMs to reduce the environmental impact of these models. These suggestions are:</p>
   <ul>
     <li><strong>Model Selection</strong>: Pre-trained models use significantly less power than training new models</li>
     <li><strong>Model Improvement</strong>: Prompt engineering, RAG, and Fine-tuning can all be used to improve 
      functionality of existing models without needed to train the models</li>
     <li><strong>Model Deployment</strong>: Using Model-as-a-Service (MaaS), the costs and energy requirements are less 
      because the MaaS infrastructure
      is typically optimized by the vendor. Model-as-a-Platform (MaaP) requires more customization and may be inefficient to 
      use. Model Parameters can also be used to optimize the model performance while minimizing the energy requirements to use. 
    </li>
    <li><strong>Model Evaluation</strong>: When using these models, users should evaluate the costs and performance in order to assess the applicability 
  of their models.
  </div>
</div>
{% endblock %}
