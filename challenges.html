<!doctype html>
<html lang="en" data-bs-theme="dark">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ai4libraries 2024 Conference - Edge-AI and FOLIO at Stanford University Libraries</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
  </head>
  <body>
  
  <nav class="navbar navbar-expand-lg bg-body-tertiary">
   <div class="container-fluid">
    <a class="navbar-brand" href="index.html">ai4libraries 2024</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link active" aria-current="page" href="index.html">Home</a>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Topics
          </a>
          <ul class="dropdown-menu">
          
            <li><a class="dropdown-item" href="index.html">Home</a></li>
          
            <li><a class="dropdown-item" href="folio.html">FOLIO Library Services Platform</a></li>
          
            <li><a class="dropdown-item" href="compound-ai.html">Compound AI Systems</a></li>
          
            <li><a class="dropdown-item" href="challenges.html">Challenges using LLMs</a></li>
          
            <li><a class="dropdown-item" href="prompt-cataloging.html">Prompt Cataloging Example</a></li>
          
            <li><a class="dropdown-item" href="upload-cataloging.html">Upload Image Cataloging Example</a></li>
          
            <li><a class="dropdown-item" href="edge-ai-next-steps.html">Edge-AI Next Steps</a></li>
          
            <li><a class="dropdown-item" href="conclusion.html">Conclusion</a></li>
          
            <li><a class="dropdown-item" href="resources.html">Resources</a></li>
          
          </ul>          
        </li>
        <li class="nav-item">
          <a class="nav-link" href="resources.html">Resources</a>
        </li>
      </ul>
    </div>
   </div>
  </nav>
  

  <div class="container">
    
     <h2>Edge-AI and FOLIO at Stanford University Libraries</h2>
     <h1>Challenges using LLMs</h1>
    
    
<div>
 <p>
   There are multiple challenges associated with using AI and  
   more specifically Large Language Models (LLMs) like <a href="https://chatgpt.com/">ChatGPT</a>
   and <a href="https://gemini.google.com/">Google's Gemini</a>, some of which can
   be minimized.
 </p>
</div>
<div class="row">
  <div class="col-4">
    <h3>Privacy Concerns in Large Language Models</h3>
     <p>
      With the widespread release of Large Language Models (LLMs) by various  organizations,
      <a href="https://arxiv.org/abs/2310.10383">significant privacy issues</a> have been observed including</p>
     <ul>
       <li><strong>Personal details in training data</strong>: Names, addresses, and financial information may
       be included in the training data for these models. </li>
      <li><strong>Logged prompts containing sensitive information</strong>: User inputs, including private 
      details, can be logged by the companies managing these LLMs. </li>
      <li><strong>Re-identification of individuals:</strong> Even in anonymized training data, individuals can potentially be
       re-identified through the model outputs and usage patterns.</li>
     </ul>
  </div>
  <div class="col-4">
   <h3>Bias</h3>
   <p>AI bias occurs when models produce outputs that reflect or 
      perpetuate existing inequalities and perspectives in the larger
      society.</p>
   <h4>Sources</h4>
   <ul>
     <li><strong>Training data</strong> -- over or under sampling underrepresented groups, biases in 
         labeling by excluding or over-representing certain categories or characteristics.
     </li>
     <li><strong>Algorithms</strong> -- based on biased data could perpetrate underlying flaws or programmers 
         introduce personal bias intentionally or unintentionally
     </li>
     <li><strong>Proxies</strong> -- Unintended consequences of using proxies for characteristics in the population</li>
     <li><strong>Cognitive </strong>-- people's experiences and preferences can introduce and favor bias or 
  weighting of outcomes or the selection of data.</li>
   </ul>
  </div>
  <div class="col-4">
   <h3>Hallucinations</h3>
   <p>
     Since the initial release of ChatGPT 3.5 in 2022, a major criticism of Large 
     Language Models (LLMs) has been the tendency of these models to fabricate factually
     incorrect statements. LLMs generate text by predicting the most likely continuation token
     based on the prompt's text, context, and model's internal weights. Unlike a deductive process, LLMs
     do not directly reference their training source material to generate responses. 
   </p>
   <h4>Types of Hallucinations</h4>
   <ul>
    <li><strong>Fact-conflicting</strong> -- 1+1 = 3</li>
    <li><strong>Input-conflicting</strong> -- LLM summarizes an article and includes details not present
     in the original article</li>
    <li><strong>Context-conflicting</strong> -- inconsistent or self-contradiction in the model's outputs 
   </ul>
   <h4>Mitigation</h4>
   <ul>
     <li><strong>Chain-of-thought (COT)</strong>: A technique prompting the model to break down its reasoning process
     into sequential steps, explaining how it arrived to its final answer.</li>
     <li><strong>One-shot and Few-shot Prompts</strong>: Techniques that provide context by offering sample responses in 
      a given format, enabling the model to infer patterns for consistency and accuracy in its
      answers.</li>
     <li><strong>Retrieval Augmented Generation (RAG)</strong>: Combines contextual examples with the prompt, grounding the
     model in factual, current material from external sources, and reducing the model's dependence on 
      outdated or incomplete information in its training data.</li>
    <li><strong>Reinforcement Learning with Human Feedback (RLHF)</strong>: A fine-tuning technique of adding direct human 
     feedback to a model's responses by rewarding factual responses and penalizing hallucinations.</li>
   </ul>
  </div>
</div>
<div class="row">
  <div class="col-4">
   <h3>Academic Fraud &amp; Copyright</h3>
    <p>
     One of the immediate concerns regarding academic fraud in the use of 
     Large Language Models (LLMs) that generate convincing and coherent text that
     students and researchers can pass off as original work. The growth and inclusion 
     of generative text into academic articles has been widespread, particularly in 
     the computer science literature (<a href="https://hai.stanford.edu/news/how-much-research-being-written-large-language-models">How Much Research Is Being Written by Large Language Models?</a>)
    </p>
   <p>
     The training of Large Language Models (LLMs) requires massive amounts of text and other 
     media that are commonly available on the open web. This content includes both copyrighted 
     and public domain material, which can lead to generative outputs from these models closely 
     resembling existing copyrighted works resulting in various lawsuits.
   </p>
  </div>
  <div class="col-4">
   <h3>Carbon Footprint</h3>
   <p>
     A real concern of Large Language Models (LLMs) is the amount of energy and water 
     required for training and deploying these models. For example, in their 2024 
     <a href="https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf">report</a>
     Google admitted that their carbon output increased over 13% year-over-year primarily due to the 
     increased energy usage of their customer-facing AI efforts, including the training and inference
     of their flagship <a href="https://gemini.google.com/">Gemini LLM</a>.
   </p>
   <p>A 2024 <a href="https://techcommunity.microsoft.com/t5/azure-architecture-blog/reducing-the-environmental-impact-of-generative-ai-a-guide-for/">report</a> by Microsoft, offers four suggestions to reduce the environmental impact of these models. 
    These suggestions are:</p>
   <ul>
     <li><strong>Model Selection</strong>: Pre-trained models use significantly less power than training new models.</li>
     <li><strong>Model Improvement</strong>: Prompt engineering, RAG, and Fine-tuning can all be used to improve 
      functionality of existing models without needing to train new models</li>
     <li><strong>Model Deployment</strong>: Using Model-as-a-Service (MaaS), the costs and energy requirements are less 
      because the MaaS infrastructure is typically optimized by the vendor.
    </li>
    <li><strong>Model Evaluation</strong>: When using these models, users should evaluate the costs and performance in 
     order to assess the applicability of their models.</li>
  </div>
</div>

    
    <div class="text-center mt-3">
      <div class="btn-group" role="group" aria-label="Previous Next Topic buttons">
      
        <a class="btn btn-warning" href="compound-ai.html">
         <i class="bi bi-arrow-left-circle"></i>
         Compound AI Systems
        </a>
       
       <button class="btn btn-primary" disabled>Challenges using LLMs</button>
      
        <a class="btn btn-success" href="prompt-cataloging.html">
         Prompt Cataloging Example
         <i class="bi bi-arrow-right-circle"></i>
        </a>
      
      </div>
    </div>
    

  </div>
  
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/umd/popper.min.js" integrity="sha384-I7E8VVD/ismYTF4hNIPjVp/Zjvgyol6VFvRkX/vR+Vc4jQkC+hVqc2pM8ODewa9r" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha384-0pUGZvbkm6XF6gxjEnlmuGrJXVbNuzT9qBBavbLwCsOGabYfZo0T0to5eqruptLy" crossorigin="anonymous"></script>
    
  </body>
</html>